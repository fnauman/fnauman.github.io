[
  {
    "objectID": "randomposts.html",
    "href": "randomposts.html",
    "title": "TIL/Random Notes",
    "section": "",
    "text": "This is a collection of random thoughts and things I’ve learned. This often reflects a work in progress, and I may update posts as I learn more.\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Python to React: Using Claude Artifacts and ChatGPT Canvas to Build Apps\n\n\n\nweb development\n\n\nreact\n\n\njavascript\n\n\nnodejs\n\n\nvite\n\n\napps\n\n\n\nLearn how to set up a modern web development environment: React, Node.js, and Vite.\n\n\n\n\n\n\nJan 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUploading datasets to Hugging Face\n\n\n\nhuggingface\n\n\ndatasets\n\n\n\nHugging Face Datasets and Datasets Hub\n\n\n\n\n\n\nJan 26, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-11-20-llms-summary/index.html",
    "href": "posts/2023-11-20-llms-summary/index.html",
    "title": "Large Language Models: A Compact Guide",
    "section": "",
    "text": "The purpose of this guide is to provide a short summary about modern Large Language Models (LLMs) from an application building perspective. I am in the process of adding more references and details to this guide, but for now, this should serve as a good starting point for anyone interested in understanding the basics of Large Language Models.\n\n\nBest models and tools I use as of Jan. 17th, 2025:\n\nCode: Claude 3.5 Sonnet (also hearing a lot about DeepSeekv3 - 93.1% of aider’s own code writes are using Deepseekv3)\nWriting: Gemini 2.0 Experimental 1206 - this has become my primary model for most use cases, but unfortunately currently only supports file attachments through AI studio interface, not the Gemini app.\nAudio: OpenAI’s GPT-4o, Gemini 2.0 Flash - both seem to havea 30 min limit unfortunately.\nPlanning: o1 by Open AI (with some input by Claude and Gemini).\nResearch: NotebookLM, Gemini Deep Research (mostly human-in-the-loop workflows where I write custom prompts, sources, etc.).\nIDEs: Windsurf, VSCode with Copilot\n\n\n\n\nA language model aims to learn the probability distribution of a sequence of words. In deep learning, typically a language model consists of the following components:\n\nTokenizer: Words, subwords, or characters need to be first converted into numerical representations. This is done by a tokenizer. Unfortunately, the community doesn’t seem to stick to universal tokenizers and many Large Language Models seem to define their own tokenizers. For instance, OpenAI uses a learned byte-pair encoding tokenizer, while T5 uses a SentencePiece tokenizer. Tokenizer is often considered a bottleneck in modern language models (and also in encoder models like BERT) because of its inabililty to adapt to:\n\nNew natural languages: for example, a model trained only on English will have trouble tokenizing a sentence in Chinese, Urdu, or Swahili.\nDomain specific languages like HTML, programming languages, etc. pose particularly difficult challenges for tokenizers since they have their own breaks, tags, etc. A similar issue also shows up with retrieval applications where it is not entirely clear how to divide\n\nEmbedding layer: The numerical representations of text are converted into dense vectors by a learned embedding layer. The size of embedding layer is typically a hyperparameter and most modern LLMs likely use an embedding size of 2048 or larger.\nSelf-attention layers: It is a fascinating concept: it allows each word or token to “attend” to all other token in the sequence. This is arguably the most important innovation in NLP in the last decade. By having very little inductive biases, Transformers are able to capture non-local relationships between tokens granted they are trained on a large enough dataset. Through multiple attention heads, these models learn multiple meanings of the same word given a context. As of this writing, all state-of-the-art Large Language Models are based on decoder-only Transformers with the exciting exceptions of RWKV-LM and Mamba.  There are other layers like LayerNorm and activations like GeLU that are part of modern architectures, but they have mostly have an empirical value - they stabilize the training process.\n\n\n\n\n\nEncoder only: Architectures like BERT are encoder only, and are often used for pre-training on a large corpus of data using a masked language modeling objective. These models can be great for tasks such as sentiment classification, named entity recognition. If you’ve heard of embedding models, they are typically also encoder only models trained using a contrastive loss.\nEncoder-decoder: For tasks like machine translation, one often needs to take an input sequence and generate an output sequence of approximately same length. This is best achieved by encoder decoder or sequence-to-sequence architectures. Examples include T5.\nDecoder only (ChatGPT, Claude, Gemini, Llama): Arguably the most popular LLM architecture currently is the decoder only architectures. Decoder models are generative by construction: they take an input (prompt) and generate a sequence of tokens. These models are often used for tasks such as question answering, summarization, and text generation. The pre-training objective for these models is causal language modeling: that is the model is trained to predict the next word in the sequence given all previous words.\n\n\n\n\nModern transformers combine the best ideas from the three big paradigms of machine learning: self-supervised learning, supervised learning, and reinforcement learning. Specifically, training a Large Language Model involves the following stages:\n\nPretraining or self-supervised training (size \\(&gt;10\\) Trillion tokens): This is arguably the largest and most compute expensive stage where the model predicts the next token. The data for this stage is typically a mix of code, math, science, fiction, etc. and the formatting is kept as is. But modern models also experiment with formatting the data in a similar way to the instruction tuning phase where a piece of text is converted to a “task - response” format.\nInstruction tuning or supervised fine-tuning (size \\(0.1-1\\) Million tokens): In this phase, the models train on a supervised task such as question answering, summarization, essay writing, etc. The data mixtures are important here: for instance, a model trained on a large fraction of code tasks will likely perform poorly on a essay writing task. For solid general purpose performance as we have grown to expect from GPT-4 class models, the data mixtures should be diverse and high quality.\nPreference tuning or reinforcement learning from human feedback (size \\(0.1-1\\) Million tokens): Tasks such as the quality of an essay are hard to judge objectively. For such tasks, one can use human preference data to fine-tune the model. The data typically consists of a pair of options for a human to choose from, and based on the preference, the model is updated. The actual process is quite involved with some algorithms requiring reward model training, etc.\nReinforcement finetuning: OpenAI’s o1 reasoning model is a special model that has received an extra stage of training that involves some sort of process reward modeling - the core concept is that the models should recieve partial credit if they are thinking in the right direction, and they can have multiple attempts at a task. The details of how this works in practice are not open source although some exciting open source work has been done recently by the Qwen team among others.\n\n\n\n\n\nPrompt Sensitivity: Due to their stochastic nature and the fact that the training data is hidden from the end user, it is often hard to predict how sensitive the model output will be if a prompt is slightly changed while retaining the same semantic meaning. This is particularly troublesome for agentic applications where the LLM is supposed to make decisions on behalf of the user. One possible way to address this is to first test out a range of prompts and then select the one that gives the best output, a process that can be automated using frameworks like DSPy.\nPlanning: Leaving out the reasoning models like OpenAI’s o1, LLMs are not able to plan ahead. That is to say, the models are unable to think through a problem first, and then build a solution. This has been labeled as the test-time-compute problem. People often report that prompting the model to “think step by step” solves this issue, but that is a myth since the model is only generating one token at a time at constant compute.\nSelf-improvement: LLMs get stuck in loops, that is they keep making the same mistakes over and over again. Although o1 and Claude 3.5 Sonnet have clearly demonstrated their ability to self-correct and that is why they perform really well on code benchmarks, in general, self-improvement remains a challenge.\nKnowing vs Understanding: When tested on counterfactual puzzles and questions, the same LLM that performs well on a wide range of tasks fails miserably. This is because the model is not able to understand the underlying concepts and instead memorizes the training data.\nVocabulary and Domain Specializations: Many domains like medicine or law have their own large set of vocabularies and concepts - words and phrases that do not appear in general language models. This necessitates the need for domain specific models.\nLong Tail: Concepts and ideas that appear less frequently on the web are unlikely to be learned by the model. While Retrieval Augmented Generation (RAG) has been proposed as a way to address this by providing LLMs relevant context before generation, it remains a hard problem since generating high quality text for rare ideas and concepts might require more “core” knowledge than what the model has."
  },
  {
    "objectID": "posts/2023-11-20-llms-summary/index.html#update",
    "href": "posts/2023-11-20-llms-summary/index.html#update",
    "title": "Large Language Models: A Compact Guide",
    "section": "",
    "text": "Best models and tools I use as of Jan. 17th, 2025:\n\nCode: Claude 3.5 Sonnet (also hearing a lot about DeepSeekv3 - 93.1% of aider’s own code writes are using Deepseekv3)\nWriting: Gemini 2.0 Experimental 1206 - this has become my primary model for most use cases, but unfortunately currently only supports file attachments through AI studio interface, not the Gemini app.\nAudio: OpenAI’s GPT-4o, Gemini 2.0 Flash - both seem to havea 30 min limit unfortunately.\nPlanning: o1 by Open AI (with some input by Claude and Gemini).\nResearch: NotebookLM, Gemini Deep Research (mostly human-in-the-loop workflows where I write custom prompts, sources, etc.).\nIDEs: Windsurf, VSCode with Copilot"
  },
  {
    "objectID": "posts/2023-11-20-llms-summary/index.html#modern-architectures-short-summary",
    "href": "posts/2023-11-20-llms-summary/index.html#modern-architectures-short-summary",
    "title": "Large Language Models: A Compact Guide",
    "section": "",
    "text": "A language model aims to learn the probability distribution of a sequence of words. In deep learning, typically a language model consists of the following components:\n\nTokenizer: Words, subwords, or characters need to be first converted into numerical representations. This is done by a tokenizer. Unfortunately, the community doesn’t seem to stick to universal tokenizers and many Large Language Models seem to define their own tokenizers. For instance, OpenAI uses a learned byte-pair encoding tokenizer, while T5 uses a SentencePiece tokenizer. Tokenizer is often considered a bottleneck in modern language models (and also in encoder models like BERT) because of its inabililty to adapt to:\n\nNew natural languages: for example, a model trained only on English will have trouble tokenizing a sentence in Chinese, Urdu, or Swahili.\nDomain specific languages like HTML, programming languages, etc. pose particularly difficult challenges for tokenizers since they have their own breaks, tags, etc. A similar issue also shows up with retrieval applications where it is not entirely clear how to divide\n\nEmbedding layer: The numerical representations of text are converted into dense vectors by a learned embedding layer. The size of embedding layer is typically a hyperparameter and most modern LLMs likely use an embedding size of 2048 or larger.\nSelf-attention layers: It is a fascinating concept: it allows each word or token to “attend” to all other token in the sequence. This is arguably the most important innovation in NLP in the last decade. By having very little inductive biases, Transformers are able to capture non-local relationships between tokens granted they are trained on a large enough dataset. Through multiple attention heads, these models learn multiple meanings of the same word given a context. As of this writing, all state-of-the-art Large Language Models are based on decoder-only Transformers with the exciting exceptions of RWKV-LM and Mamba.  There are other layers like LayerNorm and activations like GeLU that are part of modern architectures, but they have mostly have an empirical value - they stabilize the training process."
  },
  {
    "objectID": "posts/2023-11-20-llms-summary/index.html#type-of-language-models",
    "href": "posts/2023-11-20-llms-summary/index.html#type-of-language-models",
    "title": "Large Language Models: A Compact Guide",
    "section": "",
    "text": "Encoder only: Architectures like BERT are encoder only, and are often used for pre-training on a large corpus of data using a masked language modeling objective. These models can be great for tasks such as sentiment classification, named entity recognition. If you’ve heard of embedding models, they are typically also encoder only models trained using a contrastive loss.\nEncoder-decoder: For tasks like machine translation, one often needs to take an input sequence and generate an output sequence of approximately same length. This is best achieved by encoder decoder or sequence-to-sequence architectures. Examples include T5.\nDecoder only (ChatGPT, Claude, Gemini, Llama): Arguably the most popular LLM architecture currently is the decoder only architectures. Decoder models are generative by construction: they take an input (prompt) and generate a sequence of tokens. These models are often used for tasks such as question answering, summarization, and text generation. The pre-training objective for these models is causal language modeling: that is the model is trained to predict the next word in the sequence given all previous words."
  },
  {
    "objectID": "posts/2023-11-20-llms-summary/index.html#language-model-training-stages",
    "href": "posts/2023-11-20-llms-summary/index.html#language-model-training-stages",
    "title": "Large Language Models: A Compact Guide",
    "section": "",
    "text": "Modern transformers combine the best ideas from the three big paradigms of machine learning: self-supervised learning, supervised learning, and reinforcement learning. Specifically, training a Large Language Model involves the following stages:\n\nPretraining or self-supervised training (size \\(&gt;10\\) Trillion tokens): This is arguably the largest and most compute expensive stage where the model predicts the next token. The data for this stage is typically a mix of code, math, science, fiction, etc. and the formatting is kept as is. But modern models also experiment with formatting the data in a similar way to the instruction tuning phase where a piece of text is converted to a “task - response” format.\nInstruction tuning or supervised fine-tuning (size \\(0.1-1\\) Million tokens): In this phase, the models train on a supervised task such as question answering, summarization, essay writing, etc. The data mixtures are important here: for instance, a model trained on a large fraction of code tasks will likely perform poorly on a essay writing task. For solid general purpose performance as we have grown to expect from GPT-4 class models, the data mixtures should be diverse and high quality.\nPreference tuning or reinforcement learning from human feedback (size \\(0.1-1\\) Million tokens): Tasks such as the quality of an essay are hard to judge objectively. For such tasks, one can use human preference data to fine-tune the model. The data typically consists of a pair of options for a human to choose from, and based on the preference, the model is updated. The actual process is quite involved with some algorithms requiring reward model training, etc.\nReinforcement finetuning: OpenAI’s o1 reasoning model is a special model that has received an extra stage of training that involves some sort of process reward modeling - the core concept is that the models should recieve partial credit if they are thinking in the right direction, and they can have multiple attempts at a task. The details of how this works in practice are not open source although some exciting open source work has been done recently by the Qwen team among others."
  },
  {
    "objectID": "posts/2023-11-20-llms-summary/index.html#limitations-of-large-language-models",
    "href": "posts/2023-11-20-llms-summary/index.html#limitations-of-large-language-models",
    "title": "Large Language Models: A Compact Guide",
    "section": "",
    "text": "Prompt Sensitivity: Due to their stochastic nature and the fact that the training data is hidden from the end user, it is often hard to predict how sensitive the model output will be if a prompt is slightly changed while retaining the same semantic meaning. This is particularly troublesome for agentic applications where the LLM is supposed to make decisions on behalf of the user. One possible way to address this is to first test out a range of prompts and then select the one that gives the best output, a process that can be automated using frameworks like DSPy.\nPlanning: Leaving out the reasoning models like OpenAI’s o1, LLMs are not able to plan ahead. That is to say, the models are unable to think through a problem first, and then build a solution. This has been labeled as the test-time-compute problem. People often report that prompting the model to “think step by step” solves this issue, but that is a myth since the model is only generating one token at a time at constant compute.\nSelf-improvement: LLMs get stuck in loops, that is they keep making the same mistakes over and over again. Although o1 and Claude 3.5 Sonnet have clearly demonstrated their ability to self-correct and that is why they perform really well on code benchmarks, in general, self-improvement remains a challenge.\nKnowing vs Understanding: When tested on counterfactual puzzles and questions, the same LLM that performs well on a wide range of tasks fails miserably. This is because the model is not able to understand the underlying concepts and instead memorizes the training data.\nVocabulary and Domain Specializations: Many domains like medicine or law have their own large set of vocabularies and concepts - words and phrases that do not appear in general language models. This necessitates the need for domain specific models.\nLong Tail: Concepts and ideas that appear less frequently on the web are unlikely to be learned by the model. While Retrieval Augmented Generation (RAG) has been proposed as a way to address this by providing LLMs relevant context before generation, it remains a hard problem since generating high quality text for rare ideas and concepts might require more “core” knowledge than what the model has."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Farrukh Nauman",
    "section": "",
    "text": "I am Farrukh Nauman, PhD in Theoretial and Computational Astrophysics from the University of Rochester (2015). In my current job as an AI Researcher at RISE Research Institutes of Sweden AB, my work has involved large-scale AI projects in the fashion industry, LLMs for research reports, object detection applications in the farming industry, time series classification of sensor signals, etc.\n\nMy broad research interests include my recent interest in LLM applications to computational fluid dynamics applied to astrophysical flows. I have several years of experience working with many programming languages and frameworks including Python, C, FORTRAN. One particular pashion that I have is for time series forecasting and I have worked on several projects in this domain both in academia and industry."
  },
  {
    "objectID": "randomposts/2025-01-31-hf-data-upload/index.html",
    "href": "randomposts/2025-01-31-hf-data-upload/index.html",
    "title": "Uploading datasets to Hugging Face",
    "section": "",
    "text": "Uploading datasets to huggingface turned out to be harder than I initially thought."
  },
  {
    "objectID": "randomposts/2025-01-31-hf-data-upload/index.html#data-formatting",
    "href": "randomposts/2025-01-31-hf-data-upload/index.html#data-formatting",
    "title": "Uploading datasets to Hugging Face",
    "section": "Data formatting",
    "text": "Data formatting\nTo be able to view the dataset in the Hugging Face Datasets Hub, the dataset needs to be formatted in the right way. I had an image classification dataset with several target attributes. I followed the guide here. Here is how I formatted the dataset:\n./\n├── train\n│   ├── metadata.csv\n│   ├── front_2024_02_29_14_26_45.jpg\n│   ├── ...\n├── test\n│   ├── metadata.csv\n│   ├── front_2024_05_30_12_30_02.jpg\n│   ├── ...\nwhere the metadata.csv file contains the file_name as the first column and several other columns each corresponding to a target attribute. Here is a sample:\nfile_name,brand,usage,condition,type,category,price,trend,colors,cut,pattern,season,text,pilling,damage,stains,holes,smell,material\nfront_2022_12_14_08_48_42.jpg,Junkyard,Export,3,Jeans,Ladies,50-100,Denim,['Blue'],['Loose'],None,Spring,,4,,Minor,None,None,100%cotton\nfront_2023_06_29_08_22_48.jpg,Stacy,Reuse,4,Jeans,Unisex,50-100,None,['Brown'],['Tight'],None,All,,3,,None,None,None,\"98% cotton, 2% elastane\"\nWhen rendered in the dataset viewer in the Hugging Face Datasets Hub, the dataset converts the file_name to an image preview with the title image and retains the other columns. Here is the preview of the dataset:"
  },
  {
    "objectID": "randomposts/2025-01-31-hf-data-upload/index.html#what-worked",
    "href": "randomposts/2025-01-31-hf-data-upload/index.html#what-worked",
    "title": "Uploading datasets to Hugging Face",
    "section": "What worked",
    "text": "What worked\nIn the same image_dataset tutorial, they describe how to upload the dataset using the python command:\nfrom datasets import load_dataset\n\n# Load the LOCAL folder as a `huggingface/datasets` dataset\ndataset = load_dataset(\"imagefolder\", data_dir=\"./\") # `imagefolder` is a special dataset type that loads images\n\n# Upload the dataset to Hugging Face\ndataset.push_to_hub(\"fnauman/fashion-second-hand-front-only\", private=True) # `private=True` makes the dataset repo private\nI recommend first uploading the dataset as a private dataset to ensure the upload worked and the data preview works as expected. You can later make the dataset public if you wish."
  },
  {
    "objectID": "randomposts/2025-01-31-hf-data-upload/index.html#what-did-not-work",
    "href": "randomposts/2025-01-31-hf-data-upload/index.html#what-did-not-work",
    "title": "Uploading datasets to Hugging Face",
    "section": "What did not work",
    "text": "What did not work\nFollowing the instructions here, I tried using the huggingface-cli command with two variations, but it did not work.\nhuggingface-cli upload fashion-second-hand-front-only . . --repo-type dataset\nhuggingface-cli upload-large-folder fashion-second-hand-front-only --repo-type dataset . --num-workers=8\nBoth of these commands crashed and were relatively slow. I suspect it has to do with the large number of files (30,000) in the dataset."
  },
  {
    "objectID": "randomposts/2025-01-26-react-template/index.html",
    "href": "randomposts/2025-01-26-react-template/index.html",
    "title": "From Python to React: Using Claude Artifacts and ChatGPT Canvas to Build Apps",
    "section": "",
    "text": "As a seasoned Python developer, I’m used to the comfort of conda environments and the predictability of pip. But lately, I’ve been diving headfirst into the wild world of web development with React. What prompted this shift? The incredible advancements in AI tools like Claude’s new artifact feature and ChatGPT’s enhanced support for rendering HTML and React. These tools have made it ridiculously easy to generate React UIs with simple natural language prompts.\nHowever, bridging the gap between AI-generated code and a functional, maintainable application required me to master a new set of tools and concepts. The most pressing challenge? Taming the JavaScript ecosystem. This blog post chronicles my journey, focusing on setting up a solid React template using Vite while navigating the intricacies of Node.js and its package managers.\n\ncreate-react-app does not work; vite does\nMy first instinct was to reach for the familiar create-react-app. Unfortunately, as of January 2025, there are significant issues (see this GitHub issue and many others).\nVite (French for “fast”) is a next-generation build tool that significantly improves the front-end development experience. Here’s how I set up my React template using Vite:\n\nPrerequisites:\nThis guide assumes a Linux-based system (specifically Ubuntu 24.04 LTS in my case). I’ll update my experiences with Windows later.\n\n\nStep 1: Node.js and the Power of nvm\nBefore diving into Vite, we need Node.js, the runtime environment that allows us to run JavaScript outside of a browser. However, different projects might require different Node.js versions. Here’s where Node Version Manager (nvm) comes to the rescue.\n\nInstalling nvm: bash     curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash\nInstalling Node.js (LTS): bash     nvm install --lts\n\nnvm allows you to seamlessly switch between Node.js versions using a simple .nvmrc file in your project root, ensuring compatibility across your projects.\n\n\nStep 2: Initializing the React Template with Vite\nWith Node.js in place, creating a React template is a breeze:\nnpm create vite@latest my-react-app\nThis command prompts you to choose a framework (React, Vue, Svelte, etc.) and whether to use JavaScript or TypeScript. I opted for React and JavaScript. You can also use the shorthand: npm create vite@latest my-react-app -- --template react.\n\n\nStep 3: Understanding npm vs. npx\nThroughout this process, you’ll encounter both npm and npx. Here’s a simple way to distinguish them:\n\nnpm (Node Package Manager): Use it to install packages, either globally or locally within a project.\nnpx (Node Package Execute): Use it to run packages, often one-off tools or generators, without installing them permanently.\n\nFor example, we used npm create vite@latest to initialize our project because create-vite is a tool we might use again. However, for tasks like adding a shadcn component later, we’ll use npx.\n\n\nStep 4: Environment isolation - node_modules Directory\nUnlike Python’s virtual environments, Node.js relies on a project-specific node_modules directory to store dependencies. This directory can become quite large, but it guarantees that each project has its own isolated set of packages.\nKey Takeaway: Never share node_modules between projects. Always run npm install in a new project clone to populate the node_modules directory based on the package.json and package-lock.json files.\n\n\nStep 5: Embracing tailwindcss and shadcn (with a Caveat)\nModern React development often involves styling libraries like tailwindcss and component libraries like shadcn.\nImportant Note: shadcn is not an npm package but rather a collection of components that you can add to your project. It relies on Radix UI (@radix-ui/react-*) packages for its core functionality.\nIntegrating tailwindcss and shadcn can be tricky due to version conflicts. As of my writing, tailwindcss recently released a major version 4, and shadcn’s documentation hasn’t fully caught up.\nMy Solution: I’ve created a public template repository that successfully integrates tailwindcss version 3.4.17 with shadcn. It includes all the necessary configuration changes to the Vite template.\n\n\nStep 6: Organizing Your Code with Components\nFor larger projects, it’s crucial to structure your code effectively. Instead of dumping everything into src/App.tsx, create a src/components directory for your UI components. You can then import these components into App.tsx. Most LLMs seem to bundle everything in a single App.tsx by default, but you can prompt them to write modular code by separating out the components.\nHere’s a suggested folder structure:\nmy-react-app/\n├── node_modules/\n├── public/\n├── src/\n│   ├── components/\n│   │   └── Interface.tsx # Main UI code\n│   ├── App.css\n│   ├── App.tsx         # Import and use your UI components\n│   ├── index.css\n│   ├── main.tsx\n│   └── vite-env.d.ts\n├── index.html\n├── package.json\n├── package-lock.json\n├── tsconfig.json / jsconfig.json\n├── tsconfig.node.json\n└── vite.config.ts / vite.config.js\n\n\nStep 7: Running Your App\nFinally, to see your app in action, use:\nnpm run dev\nThis starts the Vite development server, typically at localhost:5173 (unlike create-react-app’s default localhost:3000).\n\n\n\nConclusion: JavaScript Environment Isolation is Great\nThe JavaScript ecosystem can feel daunting at first, especially coming from a Python background. However, with tools like Vite and a clear understanding of Node.js’s package management, setting up a modern React development environment becomes manageable.\nMy template repository provides a solid starting point, and I encourage you to explore it, adapt it, and contribute to it. As I continue my journey into web development, I’ll keep updating this blog with new insights and discoveries. Stay tuned! Github repository link: https://github.com/fnauman/clip_react\nI hope this comprehensive blog post is helpful! Let me know if you’d like any adjustments or further details on specific aspects."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Automating second-hand fashion\n\n\n\nApplied AI\n\n\nSustainable Fashion\n\n\nCircular Economy\n\n\nRISE\n\n\n\nHow can AI be used to accelerate the transition to a circular economy?\n\n\n\nFarrukh Nauman\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Intelligence as a Complex System: Lessons from Physics\n\n\n\nAritifical Intelligence\n\n\nComplex Systems\n\n\nNatural Language Processing\n\n\nLarge Language Models\n\n\n\nUnderstanding Intelligence Through the Lens of Physics\n\n\n\nFarrukh Nauman\n\n\nJun 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Models: A Compact Guide\n\n\n\nApplied AI\n\n\nNatural Language Processing\n\n\nLarge Language Models\n\n\nNotes\n\n\n\nWhat are Large Language Models? What are their limitations and common use cases?\n\n\n\nFarrukh Nauman\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-06-15-intelligence-complex/index.html",
    "href": "posts/2024-06-15-intelligence-complex/index.html",
    "title": "Intelligence as a Complex System: Lessons from Physics",
    "section": "",
    "text": "In the rapidly evolving field of artificial intelligence, there’s a growing need to understand the nuances and complexities of intelligence itself. By drawing analogies from successful physical models of complex systems, we can gain valuable insights into the nature of intelligence and the challenges we face in replicating it. This essay explores the parallels between intelligence and other complex phenomena in physics, highlighting why simplistic approaches to AI may fall short of true artificial general intelligence (AGI).\n\n\n\nJust as physical phenomena exhibit varying degrees of complexity, intelligence exists on a spectrum. Consider the following examples:\n\nTurbulence in fluid dynamics:\n\nSimple: Rayleigh–Bénard convection near the transition point.\nComplex: Plasma behavior around supermassive black holes (Reynolds number \\(\\sim 10^{20}\\)).\n\nIntelligence tasks:\n\nSimple: Grade school math problems.\nComplex: Developing groundbreaking scientific theories (e.g., the Ising model in ferromagnetism).\n\n\nThis spectrum illustrates that, like turbulence, we may not yet know if there’s an upper limit to intelligence. It also suggests that for many tasks, a simplified or “compressed” representation might suffice, explaining why some believe AGI has been achieved based on performance in limited domains. The complexity of turbulence is well characterized by the Reynolds number, but studies in the scaling laws of LLMs leaves a lot to be desired where claims of emergence are being made on simple datasets with fixed task complexity, input and output lengths. See Are Emergent Abilities of Large Language Models a Mirage? for a refreshing take on this.\n\n\n\nHistorically, scientists have fallen into the trap of reductionism – the belief that complex phenomena can be fully explained by understanding their fundamental components. Paul Dirac’s 1929 statement exemplifies this:\n\n“The underlying physical laws necessary for the mathematical theory of a large part of physics and the whole of chemistry are thus completely known.” Source\n\nHowever, just as a unified theory of fundamental forces wouldn’t explain emergent phenomena in physics, a single AI model is unlikely to capture the full spectrum of intelligence. This reductionist thinking can lead to overestimating the capabilities of current AI models, which excel in specific tasks but struggle with generalization and complex reasoning.\n\n\n\n\nDynamic nature: Intelligence is inherently non-equilibrium, dynamic, nonlinear, and high-dimensional. A static language model, no matter how advanced, cannot fully capture these aspects.\nHierarchical complexity: Like physical systems, intelligence requires flexible frameworks that allow for hierarchical representations. In physics, we have:\n\nN-particle descriptions (e.g., molecular dynamics)\nKinetic descriptions (e.g., Boltzmann, Vlasov equations)\nFluid descriptions (e.g., Navier-Stokes equations)\nMean field descriptions (e.g., filtered turbulent fields)\n\nAI research needs analogous frameworks to capture different levels of cognitive processes. However, one might argue that just like how one can use synthetic turbulence models through stochastic forcing to good effect for modeling unobserved physics across scales, GPT-like models with an analogus stochastic training process might be able to capture higher-level cognitive processes.\nMeta-frameworks for problem-solving: Current AI models lack robust strategies for approaching intractable problems. For instance, when experimental data is scarce or the physical system under consideration is extremely complex, breakthroughs in physics often came from constructing simplified models that capture the essence of complex phenomena (e.g., the Ising model for ferromagnetism).\nData limitations: Current AI models have almost no inductive biases and learn everything from the data, which has limitations:\n\nWeb-scale data has complex reasoning tasks only in their long-tail, which makes complex reasoning difficult to learn.\nDetailed reasoning steps are not available since humans only write down the final answer.\nEqual weighting of data samples is sub-optimal since only a fraction represent high-quality research and content.\nExamples from long-term scientific development are randomly distributed in the data and not systematically organized (e.g., the evolution of theories over decades).\n\nThe specific case of equal weighting of data is particularly problematic since many articles and books are written by people without adequate expertise and contain arguments without rigorous theoretical calculations and experimental results.\n\n\n\n\nDespite these challenges, AI research progresses at an unprecedented rate compared to other complex fields thanks to:\n\nImmediate access to state-of-the-art models through APIs and open-source libraries.\nEnhanced tooling for data ingestion and generation.\nThe ability to build upon existing work rapidly.\n\nContrast this with research in turbulence or computational fluid dynamics, where reproducing results can take years because of:\n\nComplex, million-line codebases in C, C++, Fortran.\nLimited access to high-performance computing.\nLack of open data sharing practices, and detailed model descriptions for reproducibility.\n\nAs an example, consider the difficulty in generating \\(1000\\) time steps for a turbulent flow simulation and contrast it with generating \\(1000\\) tokens in language models through the interface, API or locally hosted models.\n\n\n\nThe very idea of artificial “intelligence singularity” or its opposite is problematic from a scientific perspective where singularities are an indication that our theory is invalid at those scales. The fact that we so readily discuss “singularities” in the context of intelligence might indicate fundamental limitations in our current models of cognition and AI.\nWhile the rapid progress in AI is exciting, we must approach claims of AGI with caution. By viewing intelligence through the lens of complex systems, we gain a more nuanced understanding of the challenges ahead. Just as physicists continue to grapple with phenomena like turbulence, AI researchers must embrace the multifaceted nature of intelligence, developing new frameworks and approaches to capture its full complexity."
  },
  {
    "objectID": "posts/2024-06-15-intelligence-complex/index.html#introduction",
    "href": "posts/2024-06-15-intelligence-complex/index.html#introduction",
    "title": "Intelligence as a Complex System: Lessons from Physics",
    "section": "",
    "text": "In the rapidly evolving field of artificial intelligence, there’s a growing need to understand the nuances and complexities of intelligence itself. By drawing analogies from successful physical models of complex systems, we can gain valuable insights into the nature of intelligence and the challenges we face in replicating it. This essay explores the parallels between intelligence and other complex phenomena in physics, highlighting why simplistic approaches to AI may fall short of true artificial general intelligence (AGI)."
  },
  {
    "objectID": "posts/2024-06-15-intelligence-complex/index.html#the-complexity-spectrum",
    "href": "posts/2024-06-15-intelligence-complex/index.html#the-complexity-spectrum",
    "title": "Intelligence as a Complex System: Lessons from Physics",
    "section": "",
    "text": "Just as physical phenomena exhibit varying degrees of complexity, intelligence exists on a spectrum. Consider the following examples:\n\nTurbulence in fluid dynamics:\n\nSimple: Rayleigh–Bénard convection near the transition point.\nComplex: Plasma behavior around supermassive black holes (Reynolds number \\(\\sim 10^{20}\\)).\n\nIntelligence tasks:\n\nSimple: Grade school math problems.\nComplex: Developing groundbreaking scientific theories (e.g., the Ising model in ferromagnetism).\n\n\nThis spectrum illustrates that, like turbulence, we may not yet know if there’s an upper limit to intelligence. It also suggests that for many tasks, a simplified or “compressed” representation might suffice, explaining why some believe AGI has been achieved based on performance in limited domains. The complexity of turbulence is well characterized by the Reynolds number, but studies in the scaling laws of LLMs leaves a lot to be desired where claims of emergence are being made on simple datasets with fixed task complexity, input and output lengths. See Are Emergent Abilities of Large Language Models a Mirage? for a refreshing take on this."
  },
  {
    "objectID": "posts/2024-06-15-intelligence-complex/index.html#the-danger-of-hype-reductionist-thinking",
    "href": "posts/2024-06-15-intelligence-complex/index.html#the-danger-of-hype-reductionist-thinking",
    "title": "Intelligence as a Complex System: Lessons from Physics",
    "section": "",
    "text": "Historically, scientists have fallen into the trap of reductionism – the belief that complex phenomena can be fully explained by understanding their fundamental components. Paul Dirac’s 1929 statement exemplifies this:\n\n“The underlying physical laws necessary for the mathematical theory of a large part of physics and the whole of chemistry are thus completely known.” Source\n\nHowever, just as a unified theory of fundamental forces wouldn’t explain emergent phenomena in physics, a single AI model is unlikely to capture the full spectrum of intelligence. This reductionist thinking can lead to overestimating the capabilities of current AI models, which excel in specific tasks but struggle with generalization and complex reasoning."
  },
  {
    "objectID": "posts/2024-06-15-intelligence-complex/index.html#key-aspects-of-intelligence-as-a-complex-system",
    "href": "posts/2024-06-15-intelligence-complex/index.html#key-aspects-of-intelligence-as-a-complex-system",
    "title": "Intelligence as a Complex System: Lessons from Physics",
    "section": "",
    "text": "Dynamic nature: Intelligence is inherently non-equilibrium, dynamic, nonlinear, and high-dimensional. A static language model, no matter how advanced, cannot fully capture these aspects.\nHierarchical complexity: Like physical systems, intelligence requires flexible frameworks that allow for hierarchical representations. In physics, we have:\n\nN-particle descriptions (e.g., molecular dynamics)\nKinetic descriptions (e.g., Boltzmann, Vlasov equations)\nFluid descriptions (e.g., Navier-Stokes equations)\nMean field descriptions (e.g., filtered turbulent fields)\n\nAI research needs analogous frameworks to capture different levels of cognitive processes. However, one might argue that just like how one can use synthetic turbulence models through stochastic forcing to good effect for modeling unobserved physics across scales, GPT-like models with an analogus stochastic training process might be able to capture higher-level cognitive processes.\nMeta-frameworks for problem-solving: Current AI models lack robust strategies for approaching intractable problems. For instance, when experimental data is scarce or the physical system under consideration is extremely complex, breakthroughs in physics often came from constructing simplified models that capture the essence of complex phenomena (e.g., the Ising model for ferromagnetism).\nData limitations: Current AI models have almost no inductive biases and learn everything from the data, which has limitations:\n\nWeb-scale data has complex reasoning tasks only in their long-tail, which makes complex reasoning difficult to learn.\nDetailed reasoning steps are not available since humans only write down the final answer.\nEqual weighting of data samples is sub-optimal since only a fraction represent high-quality research and content.\nExamples from long-term scientific development are randomly distributed in the data and not systematically organized (e.g., the evolution of theories over decades).\n\nThe specific case of equal weighting of data is particularly problematic since many articles and books are written by people without adequate expertise and contain arguments without rigorous theoretical calculations and experimental results."
  },
  {
    "objectID": "posts/2024-06-15-intelligence-complex/index.html#the-unique-pace-of-ai-development",
    "href": "posts/2024-06-15-intelligence-complex/index.html#the-unique-pace-of-ai-development",
    "title": "Intelligence as a Complex System: Lessons from Physics",
    "section": "",
    "text": "Despite these challenges, AI research progresses at an unprecedented rate compared to other complex fields thanks to:\n\nImmediate access to state-of-the-art models through APIs and open-source libraries.\nEnhanced tooling for data ingestion and generation.\nThe ability to build upon existing work rapidly.\n\nContrast this with research in turbulence or computational fluid dynamics, where reproducing results can take years because of:\n\nComplex, million-line codebases in C, C++, Fortran.\nLimited access to high-performance computing.\nLack of open data sharing practices, and detailed model descriptions for reproducibility.\n\nAs an example, consider the difficulty in generating \\(1000\\) time steps for a turbulent flow simulation and contrast it with generating \\(1000\\) tokens in language models through the interface, API or locally hosted models."
  },
  {
    "objectID": "posts/2024-06-15-intelligence-complex/index.html#conclusion",
    "href": "posts/2024-06-15-intelligence-complex/index.html#conclusion",
    "title": "Intelligence as a Complex System: Lessons from Physics",
    "section": "",
    "text": "The very idea of artificial “intelligence singularity” or its opposite is problematic from a scientific perspective where singularities are an indication that our theory is invalid at those scales. The fact that we so readily discuss “singularities” in the context of intelligence might indicate fundamental limitations in our current models of cognition and AI.\nWhile the rapid progress in AI is exciting, we must approach claims of AGI with caution. By viewing intelligence through the lens of complex systems, we gain a more nuanced understanding of the challenges ahead. Just as physicists continue to grapple with phenomena like turbulence, AI researchers must embrace the multifaceted nature of intelligence, developing new frameworks and approaches to capture its full complexity."
  },
  {
    "objectID": "projects/2023-12-16-second-hand-fashion/index.html",
    "href": "projects/2023-12-16-second-hand-fashion/index.html",
    "title": "Automating second-hand fashion",
    "section": "",
    "text": "Textile industry is one of the biggest contributors to global pollution and green house emissions. Some statistics from the Nordic council of ministers report 2023:\n\nIt accounts for up to 10% of global green house gas emssions McKinsey 2020.\nThe water use of the textile industry exceeds 215 trillion liters per year.\nIn Nordic countries, the annual consumption textiles per capita is between 13.5 and 16 kgs.\nSynthetic fiber production, which are harder to recycle than natural fibers, has increased from less than 20% of the global fiber production to 60% today.\nLess than 1% of textiles are recycled every year.\nMany garments are only used 7-8 times before being discarded. If each garment could be worn twice as much, the emissions from the textile industry would be reduced by nearly half.\n\nThese alarming trends and statistics have forced the European Union to propose Extended Producer Responsibility (EPR) for textiles. The EPR is a policy approach that makes the producer responsible for the entire life cycle of the product, including the management of the product after its end-of-life. The EPR is expected to be implemented in the EU by 2025. See here for more details.\n\n\nAI has been used in many applications in the fashion industry including product recommendation, visual search, virtual try-on, and trend forecasting. Second-hand fashion, on the other hand, remains almost exclusively manual. The sorting and grading of second-hand clothing is a labor-intensive process that requires a lot of time and effort. The lack of automation in this sector is a major bottleneck in the transition to a circular economy.\nI am fortunate to be involved in two large projects aiming to automate the second-hand fashion industry. The first project is AI for Resource-Efficient Circular Fashion funded by the Swedish Innovation Agency, Vinnova. The second project is funded by the EU: Increasing Circularity and Sustainability in Textiles and Clothing in Europe.\n\n\n\nSorting is a multi-step process that involves the following steps:\n\nPre-sorting: Separate shoes, household textiles like bedsheets and curtains, and other non-fashion items from the fashion items.\nSorting fashion clothes [THIS PROJECT]: Predict various attributes of the clothing items and sort them for:\n\nReuse: Items that are in good condition and can be sold as-is. Reuse is the most sustainable option and has further sub-categories:\n\nSell in Sweden.\nSell outside Sweden or export.\n\nRepair: Items in need of repairs, but are otherwise reusable.\nRecycle: Items made of recyclable materials like 100% cotton.\nLandfill: Items that are in extremely poor condition and cannot be reused or recycled.\n\nFine sorting: This is the sorting that is most relevant to second-hand retailers that sell in-store and online. Their goal is to take the chunk of reusable clothing items and then decide which items to sell at what price and in what location. We do not address this in our project directly although our sorting model can be used to support this process.\n\nIn addition to this, clothes must be handled manually when they first arrive at the facility in large containers. Currently, no known technology exists that can fully automate this step although exciting advances in the field of robotics are being made.\n\n\n\nThe first major challenge that any project aiming to automate the second-hand fashion industry faces is the lack of data. Existing “foundation AI models” are largely biased towards first-hand fashion since that is the kind of data that is readily available on the internet. For instance, these models are incapable of recognizing the wear and tear of second-hand clothing since first-hand fashion images are usually of pristine quality.\nIn the Vinnova project that I am leading from RISE, we are developing a novel dataset with 30,000 used clothing items in partnership with Wargön Innovation. The first version of the dataset has already been released:\n\nDataset v1, 3000 clothing items: Zenodo link.\n\nThe dataset has been released under a permissive CC-BY 4.0 license that allows commercial use given that the authors are properly cited.\nFurthermore, we are developing AI models to recognize damage on clothes and to grade them according to their quality. The scope of ongoing projects is not full automation, but to instead provide a “decision support tool”. A decision support tool is supposed to assist the human operator in making the final decision by judging the cloth condition, assessing the brand quality and how it compares with other brands in the market, and finally, estimating the best use case for the item.\n\n\n\nWe have identified the following challenges and opportunities in the second-hand fashion industry:\n\nData: While our dataset of 30,000 clothing items is the largest of its kind, it is still not large enough to train a deep learning model of the “foundation model” kind. Instead, we must resort to using existing foundation models and finetune them with this data. What makes this particularly challenging is that for first-hand fashion, training a model on, for example, pink T-shirts and black pants is sufficient, but for second-hand fashion, one must be able to distinguish between a pink T-shirt that is in good condition and one that is in poor condition. In other words, we need a dataset large enough to contain different degrees of damage to clothes. One major problem with lack of data will be addressed by the introduction of the digital product passport that aims to preserve the data about a product throughout its life cycle.\nAnnotations: Similar to the subjectivity of language annotations, the annotations of second-hand clothing items are often specific to the annotators and the scope of the facility they are working for. For instance, Wargön Innovation works with the Swedish Red Cross and does not directly price the clothing items. In contrast, other sorting facilities like Myrorna and Björkåfrihet price the items to be sold in their own stores. This means that the annotations are not only subjective, but also specific to the business model of the sorting facility.\nRobotics: The second-hand fashion industry is still almost exclusively manual. With the recent advances in robotics, there is an exciting opportunity to fully automate the entire sorting process from the pre-sorting step to the fine sorting step.\n\n\n\n\nThe second-hand fashion industry is ripe for disruption. With the increase in global awareness about the environmental impact of the textile industry, the second-hand fashion retail is expected to grow exponentially. Nonetheless, the industry is still largely manual and lacks large scale datasets and AI models. The introduction of the digital product passport and extended producer responsibility are likely to accelerate the automation of the second-hand fashion industry. Most players in this sector are volunteer run small businesses that lack the resources to invest in AI and robotics. With project like ours, we hope to make the technology accessible to all players."
  },
  {
    "objectID": "projects/2023-12-16-second-hand-fashion/index.html#ai-in-the-fashion-industry",
    "href": "projects/2023-12-16-second-hand-fashion/index.html#ai-in-the-fashion-industry",
    "title": "Automating second-hand fashion",
    "section": "",
    "text": "AI has been used in many applications in the fashion industry including product recommendation, visual search, virtual try-on, and trend forecasting. Second-hand fashion, on the other hand, remains almost exclusively manual. The sorting and grading of second-hand clothing is a labor-intensive process that requires a lot of time and effort. The lack of automation in this sector is a major bottleneck in the transition to a circular economy.\nI am fortunate to be involved in two large projects aiming to automate the second-hand fashion industry. The first project is AI for Resource-Efficient Circular Fashion funded by the Swedish Innovation Agency, Vinnova. The second project is funded by the EU: Increasing Circularity and Sustainability in Textiles and Clothing in Europe."
  },
  {
    "objectID": "projects/2023-12-16-second-hand-fashion/index.html#sorting",
    "href": "projects/2023-12-16-second-hand-fashion/index.html#sorting",
    "title": "Automating second-hand fashion",
    "section": "",
    "text": "Sorting is a multi-step process that involves the following steps:\n\nPre-sorting: Separate shoes, household textiles like bedsheets and curtains, and other non-fashion items from the fashion items.\nSorting fashion clothes [THIS PROJECT]: Predict various attributes of the clothing items and sort them for:\n\nReuse: Items that are in good condition and can be sold as-is. Reuse is the most sustainable option and has further sub-categories:\n\nSell in Sweden.\nSell outside Sweden or export.\n\nRepair: Items in need of repairs, but are otherwise reusable.\nRecycle: Items made of recyclable materials like 100% cotton.\nLandfill: Items that are in extremely poor condition and cannot be reused or recycled.\n\nFine sorting: This is the sorting that is most relevant to second-hand retailers that sell in-store and online. Their goal is to take the chunk of reusable clothing items and then decide which items to sell at what price and in what location. We do not address this in our project directly although our sorting model can be used to support this process.\n\nIn addition to this, clothes must be handled manually when they first arrive at the facility in large containers. Currently, no known technology exists that can fully automate this step although exciting advances in the field of robotics are being made."
  },
  {
    "objectID": "projects/2023-12-16-second-hand-fashion/index.html#ai-powered-sorting",
    "href": "projects/2023-12-16-second-hand-fashion/index.html#ai-powered-sorting",
    "title": "Automating second-hand fashion",
    "section": "",
    "text": "The first major challenge that any project aiming to automate the second-hand fashion industry faces is the lack of data. Existing “foundation AI models” are largely biased towards first-hand fashion since that is the kind of data that is readily available on the internet. For instance, these models are incapable of recognizing the wear and tear of second-hand clothing since first-hand fashion images are usually of pristine quality.\nIn the Vinnova project that I am leading from RISE, we are developing a novel dataset with 30,000 used clothing items in partnership with Wargön Innovation. The first version of the dataset has already been released:\n\nDataset v1, 3000 clothing items: Zenodo link.\n\nThe dataset has been released under a permissive CC-BY 4.0 license that allows commercial use given that the authors are properly cited.\nFurthermore, we are developing AI models to recognize damage on clothes and to grade them according to their quality. The scope of ongoing projects is not full automation, but to instead provide a “decision support tool”. A decision support tool is supposed to assist the human operator in making the final decision by judging the cloth condition, assessing the brand quality and how it compares with other brands in the market, and finally, estimating the best use case for the item."
  },
  {
    "objectID": "projects/2023-12-16-second-hand-fashion/index.html#challenges-and-opportunities",
    "href": "projects/2023-12-16-second-hand-fashion/index.html#challenges-and-opportunities",
    "title": "Automating second-hand fashion",
    "section": "",
    "text": "We have identified the following challenges and opportunities in the second-hand fashion industry:\n\nData: While our dataset of 30,000 clothing items is the largest of its kind, it is still not large enough to train a deep learning model of the “foundation model” kind. Instead, we must resort to using existing foundation models and finetune them with this data. What makes this particularly challenging is that for first-hand fashion, training a model on, for example, pink T-shirts and black pants is sufficient, but for second-hand fashion, one must be able to distinguish between a pink T-shirt that is in good condition and one that is in poor condition. In other words, we need a dataset large enough to contain different degrees of damage to clothes. One major problem with lack of data will be addressed by the introduction of the digital product passport that aims to preserve the data about a product throughout its life cycle.\nAnnotations: Similar to the subjectivity of language annotations, the annotations of second-hand clothing items are often specific to the annotators and the scope of the facility they are working for. For instance, Wargön Innovation works with the Swedish Red Cross and does not directly price the clothing items. In contrast, other sorting facilities like Myrorna and Björkåfrihet price the items to be sold in their own stores. This means that the annotations are not only subjective, but also specific to the business model of the sorting facility.\nRobotics: The second-hand fashion industry is still almost exclusively manual. With the recent advances in robotics, there is an exciting opportunity to fully automate the entire sorting process from the pre-sorting step to the fine sorting step."
  },
  {
    "objectID": "projects/2023-12-16-second-hand-fashion/index.html#conclusion",
    "href": "projects/2023-12-16-second-hand-fashion/index.html#conclusion",
    "title": "Automating second-hand fashion",
    "section": "",
    "text": "The second-hand fashion industry is ripe for disruption. With the increase in global awareness about the environmental impact of the textile industry, the second-hand fashion retail is expected to grow exponentially. Nonetheless, the industry is still largely manual and lacks large scale datasets and AI models. The introduction of the digital product passport and extended producer responsibility are likely to accelerate the automation of the second-hand fashion industry. Most players in this sector are volunteer run small businesses that lack the resources to invest in AI and robotics. With project like ours, we hope to make the technology accessible to all players."
  }
]
Ordinary Least Squares (OLS) is the standard approach to regression problems with its roots going back to [early 19th century](https://en.wikipedia.org/wiki/Least_squares). OLS works well when the terms in the linear expansion are independent (no interactions), the variance is nearly the same for each data point. Real data often contains outliers that ruin the fit. To circumvent this, one can regularize the coefficients such that the coefficients corresponding to largest deviations shrink to zero.

Lasso solves the constrained optimization problem:

$$||y - \beta X||^2$$

subject to the constraint that

$$||\beta||_1 < t$$

where \( t )\ is some constant, and \( \beta \) is the feature matrix. One can also use the L2 norm: \( ||\beta||_2^2 )\ known as Ridge regression to deal with outliers but Lasso is often the preferred algorithm as it is a **embedded** method for [feature selection](https://sebastianraschka.com/faq/docs/feature_sele_categories.html). This helps in getting rid of unimportant features when making a prediction.

For low-dimensional 'small' data, Lasso often outperforms more sophisticated algorithms like random forests, gradient boosted trees and neural networks. This is primarily because Lasso does not require lots of data to perform well unlike these other algorithms. Moreover, Lasso only requires the tuning of only one hyperparameter: \( \alpha \), with practically no regularization in the limit of vanishingly small \( \alpha \) while the other limit is extreme regularization and most feature coefficients will be set to zero.

In [scikit-learn](https://scikit-learn.org/stable/), using Lasso is easy:
```python
from sklearn.linear_model import Lasso
mod = Lasso(alpha=0.01)
# Train
mod.fit(X_train, y_train)
# Predict
y_pred = mod.predict(X_test)
```
